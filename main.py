#!/usr/bin/env python3
"""
Amazon Product Crawler - Main Application
H·ªá th·ªëng crawl v√† theo d√µi s·∫£n ph·∫©m Amazon
"""

import asyncio
import sys
import signal
import uvicorn
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from config.settings import settings
from database.connection import create_tables, init_default_settings
from scheduler.crawler_scheduler import start_scheduler, stop_scheduler
from utils.logger import get_logger

logger = get_logger(__name__)

class AmazonCrawlerApp:
    def __init__(self):
        self.scheduler_running = False
        
    async def initialize(self):
        """Initialize the application"""
        try:
            logger.info("Starting Amazon Product Crawler...")
            
            # Initialize database
            logger.info("Initializing database...")
            create_tables()
            init_default_settings()
            
            # Start scheduler
            logger.info("Starting crawler scheduler...")
            await start_scheduler()
            self.scheduler_running = True
            
            logger.info("Application initialized successfully!")
            
        except Exception as e:
            logger.error(f"‚ùå Failed to initialize application: {e}")
            raise
    
    async def cleanup(self):
        """Cleanup on shutdown"""
        try:
            logger.info("Shutting down Amazon Crawler...")
            
            if self.scheduler_running:
                await stop_scheduler()
                self.scheduler_running = False
            
            logger.info("Cleanup completed!")
            
        except Exception as e:
            logger.error(f"‚ùå Error during cleanup: {e}")
    
    def setup_signal_handlers(self):
        """Setup signal handlers for graceful shutdown"""
        def signal_handler(signum, frame):
            logger.info(f"üì° Received signal {signum}, initiating graceful shutdown...")
            asyncio.create_task(self.cleanup())
            sys.exit(0)
        
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

async def run_web_server():
    """Run the FastAPI web server"""
    try:
        # Initialize app
        app = AmazonCrawlerApp()
        await app.initialize()
        
        # Setup signal handlers
        app.setup_signal_handlers()
        
        # Start web server
        logger.info(f"Starting web server on http://{settings.DASHBOARD_HOST}:{settings.DASHBOARD_PORT}")
        
        config = uvicorn.Config(
            "api.main:app",
            host=settings.DASHBOARD_HOST,
            port=settings.DASHBOARD_PORT,
            log_level="info",
            reload=False
        )
        
        server = uvicorn.Server(config)
        await server.serve()
        
    except KeyboardInterrupt:
        logger.info("üì° Received keyboard interrupt")
    except Exception as e:
        logger.error(f"‚ùå Error running web server: {e}")
    finally:
        if 'app' in locals():
            await app.cleanup()

async def run_crawler_only():
    """Run only the crawler scheduler without web interface"""
    try:
        # Initialize app
        app = AmazonCrawlerApp()
        await app.initialize()
        
        # Setup signal handlers
        app.setup_signal_handlers()
        
        logger.info("üîÑ Crawler scheduler is running. Press Ctrl+C to stop.")
        
        # Keep running
        while True:
            await asyncio.sleep(60)  # Check every minute
            
    except KeyboardInterrupt:
        logger.info("üì° Received keyboard interrupt")
    except Exception as e:
        logger.error(f"‚ùå Error running crawler: {e}")
    finally:
        await app.cleanup()

def show_help():
    """Show help message"""
    print("""
üîπ Amazon Product Crawler - H·ªá th·ªëng crawl v√† theo d√µi s·∫£n ph·∫©m Amazon

üìã S·ª≠ d·ª•ng:
    python main.py [command]

üîß Commands:
    web         - Ch·∫°y web server v·ªõi dashboard (m·∫∑c ƒë·ªãnh)
    crawler     - Ch·ªâ ch·∫°y crawler scheduler (kh√¥ng c√≥ web interface)
    setup       - Thi·∫øt l·∫≠p database v√† c·∫•u h√¨nh ban ƒë·∫ßu
    test        - Test crawl m·ªôt ASIN m·∫´u
    help        - Hi·ªÉn th·ªã th√¥ng tin n√†y

üìö V√≠ d·ª•:
    python main.py                    # Ch·∫°y full web server
    python main.py web                # Ch·∫°y web server  
    python main.py crawler            # Ch·ªâ ch·∫°y crawler
    python main.py test B019QZBS10    # Test crawl ASIN
    
üåê Dashboard: http://{host}:{port}
üìß H·ªó tr·ª£: Xem README.md ƒë·ªÉ bi·∫øt th√™m chi ti·∫øt
    """.format(host=settings.DASHBOARD_HOST, port=settings.DASHBOARD_PORT))

async def setup_database():
    """Setup database and initial configuration"""
    try:
        logger.info("Setting up database...")
        create_tables()
        init_default_settings()
        
        # Add sample ASIN for testing
        from scheduler.crawler_scheduler import add_asin
        sample_asin = "B019QZBS10"  # ASIN from the screenshot
        
        logger.info(f"Adding sample ASIN: {sample_asin}")
        success = await add_asin(sample_asin, "daily", "Sample product for testing")
        
        if success:
            logger.info("Sample ASIN added successfully!")
        else:
            logger.info("Sample ASIN already exists or failed to add")
        
        logger.info("Database setup completed!")
        
        # Show next steps
        print(f"""
üéâ Setup ho√†n t·∫•t!

üîÑ B∆∞·ªõc ti·∫øp theo:
1. C·∫•u h√¨nh th√¥ng b√°o (t√πy ch·ªçn):
   - Telegram: Th√™m TELEGRAM_BOT_TOKEN v√† TELEGRAM_CHAT_ID v√†o .env
   - Discord: Th√™m DISCORD_WEBHOOK_URL v√†o .env
   - Email: C·∫•u h√¨nh SMTP trong .env

2. Ch·∫°y ·ª©ng d·ª•ng:
   python main.py web

3. Truy c·∫≠p dashboard:
   http://{settings.DASHBOARD_HOST}:{settings.DASHBOARD_PORT}

üìã C√°c t√≠nh nƒÉng ch√≠nh:
- ‚úÖ Crawl s·∫£n ph·∫©m Amazon theo ASIN
- ‚úÖ Theo d√µi thay ƒë·ªïi gi√°, ƒë√°nh gi√°, t·ªìn kho
- ‚úÖ Th√¥ng b√°o qua Telegram/Discord/Email
- ‚úÖ Dashboard web ƒë·ªÉ qu·∫£n l√Ω
- ‚úÖ L·∫≠p l·ªãch crawl t·ª± ƒë·ªông h√†ng ng√†y
        """)
        
    except Exception as e:
        logger.error(f"‚ùå Error setting up database: {e}")
        raise

async def test_crawl(asin: str = None):
    """Test crawl a single ASIN"""
    if not asin:
        asin = "B019QZBS10"  # Default test ASIN
    
    try:
        logger.info(f"Testing crawl for ASIN: {asin}")
        
        from crawler.amazon_crawler import crawl_single_product
        result = crawl_single_product(asin)
        
        if result.get('crawl_success'):
            logger.info("Test crawl successful!")
            print(f"""
üìä K·∫øt qu·∫£ test crawl:
- ASIN: {result.get('asin')}
- Ti√™u ƒë·ªÅ: {result.get('title', 'N/A')[:100]}...
- Gi√° b√°n: ${result.get('sale_price', 'N/A')}
- Gi√° ni√™m y·∫øt: ${result.get('list_price', 'N/A')}
- ƒê√°nh gi√°: {result.get('rating', 'N/A')}/5 ({result.get('rating_count', 'N/A')} reviews)
- T·ªìn kho: {result.get('inventory_status', 'N/A')}
- Amazon's Choice: {'C√≥' if result.get('amazon_choice') else 'Kh√¥ng'}
            """)
        else:
            logger.error(f"‚ùå Test crawl failed: {result.get('crawl_error', 'Unknown error')}")
            
    except Exception as e:
        logger.error(f"‚ùå Error during test crawl: {e}")

def create_env_file():
    """Create sample .env file"""
    env_file = Path(".env")
    
    if env_file.exists():
        logger.info(".env file already exists")
        return
    
    env_content = """# Amazon Crawler Configuration

# Database
DATABASE_URL=sqlite:///./amazon_crawler.db

# Crawler Settings
CRAWLER_DELAY=3
MAX_RETRIES=3
TIMEOUT=30
HEADLESS_BROWSER=true

# Scheduler
SCHEDULER_TIMEZONE=Asia/Ho_Chi_Minh
DAILY_CRAWL_TIME=09:00

# Dashboard
DASHBOARD_HOST=127.0.0.1
DASHBOARD_PORT=8000

# Telegram Notifications (Optional)
TELEGRAM_BOT_TOKEN=
TELEGRAM_CHAT_ID=

# Discord Notifications (Optional)
DISCORD_WEBHOOK_URL=

# Email Notifications (Optional)
EMAIL_SMTP_SERVER=
EMAIL_SMTP_PORT=587
EMAIL_USERNAME=
EMAIL_PASSWORD=
"""
    
    env_file.write_text(env_content)
    logger.info("Created .env file with default configuration")

async def main():
    """Main application entry point"""
    # Create .env file if it doesn't exist
    create_env_file()
    
    # Parse command line arguments
    command = sys.argv[1] if len(sys.argv) > 1 else "web"
    
    try:
        if command == "help" or command == "--help" or command == "-h":
            show_help()
            
        elif command == "setup":
            await setup_database()
            
        elif command == "test":
            test_asin = sys.argv[2] if len(sys.argv) > 2 else None
            await test_crawl(test_asin)
            
        elif command == "crawler":
            await run_crawler_only()
            
        elif command == "web" or command == "server":
            await run_web_server()
            
        else:
            logger.error(f"‚ùå Unknown command: {command}")
            show_help()
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.info("Goodbye!")
    except Exception as e:
        logger.error(f"Application error: {e}")
        sys.exit(1)

if __name__ == "__main__":
    # Run the application
    asyncio.run(main()) 